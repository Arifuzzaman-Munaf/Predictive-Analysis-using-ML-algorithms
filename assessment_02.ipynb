{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ItWhqRu49r9_"
      },
      "source": [
        "# Using the CRISP-DM Method for MLN 601 Machine Learning\n",
        "# Assessment 2: Classification \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ctR3Nfz9r-D"
      },
      "source": [
        "# 1. Stage One - Determine Business Objectives and Assess the Situation  <a class=\"anchor\"></a>\n",
        "The traditional process of winemaking relies heavily on subjective, time-consuming and expensive sensory analysis through human experts to certify quality. This assessment often occurs at the final stage of production highlighting significant financial risk if a batch is deemed unsatisfactory (Nebot et al., 2015). This project confronts this challenge by leveraging machine learning to create a proactive and data-driven quality control system.\n",
        "\n",
        "\n",
        "The objective of this project is to develop a robust binary classification model capable of predicting wine quality with high accuracy. The analysis will utilize the well-regarded Wine Quality dataset from the UCI Machine Learning Repository which contains detailed physicochemical measurements. To frame this as a classification problem, the original quality score (ranging from 0 to 10) is transformed into a categorical variable such as wines with a score below 6 are labeled as 'low' quality (1), and those with a score of 6 or above are labeled as 'high' quality (0).\n",
        "\n",
        "\n",
        "By training prediction algorithms on this data, the analysis aims to uncover the key chemical indicators that distinguish high-quality wine from its lower-quality counterparts. The ultimate value of the resulting model will be measured by its predictive power and assessed through rigorous evaluation metrics like accuracy, precision, recall and AUC-ROC curve. A successful model would provide winemakers with an objective tool to forecast quality early, enabling timely interventions and minimizing the risk of costly production failures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Success Criteria\n",
        "The success criteria for this project is fundamentally tied to the statistical power and reliability of the classification model. To ensure the final model is robust enough for practical application, a competitive evaluation of multiple algorithms and parameter sets will be conducted. The \"champion\" model will be selected based on its superior performance against two specific critical metrics:\n",
        "\n",
        "\n",
        "**Primary Metric (F1-Score):** An F1-Score of $ \\ge 0.70 $ for the 'low' quality (1) class is the most critical hurdle. This metric is chosen because it effectively manages the business trade-offs. A False Negative (failing to detect a low-quality batch) is the most costly error potentially leading to wasted resources and reputational damage. A False Positive (flagging a good batch for review) is less costly but still inefficient. The F1-Score ensures the model finds an optimal balance by maximizing the detection of bad batches while minimizing unnecessary interventions.\n",
        "\n",
        "\n",
        "**Overall Performance (AUC-ROC):** An AUC-ROC score of $ \\ge 0.80 $ is required to confirm the generalization capability of the model. This metric provides a holistic assessment of how well the model can distinguish between high and low-quality wines across all possible decision thresholds. A high AUC value reflects that the model has discovered important and underlying trends in the data rather than an ability to perform well at only a particular and arbitrary cutoff value.\n",
        "\n",
        "\n",
        "The model with the highest F1-Score among the considered models that also exceeds the minimum level of AUC-ROC will be considered as the best solution. After selecting the champion model based on its performance, the next critical step is to analyze its interpretability by extracting feature importance. This activity considers the variables of physicochemistry one by one and then ranks the variables based on their contribution to the final quality prediction. Success in this case implies providing the best 3-5 variables that always generate a classification of a wine. For example, the model might reveal that low alcohol content combined with high volatile acidity are the most powerful predictors of a \"low quality\" rating. This elevates the model from a simple predictive tool to a diagnostic system. It gives winemakers a fine-grained and data-driven answer to consider a batch is at risk, so that they can target intervention actions on specific chemical characteristics and eventually improve the production process end to end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q7RZpV__9r-E"
      },
      "source": [
        "## 1.2 Assess the Current Situation<a class=\"anchor\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zv2qPfF99r-G"
      },
      "source": [
        "This project is supported by a well-defined set of resources ensuring a smooth and effective workflow from data analysis to final reporting.\n",
        "* **Personnel and Research**\n",
        "The project will be executed by Md. Arifuzzaman Munaf, a postgraduate student specializing in Advanced Artificial Intelligence. This role encompasses responsibility for the entire project lifecycle including data preprocessing, exploratory analysis, model development, rigorous evaluation, and final documentation. The analysis will be supplemented by existing academic literature and research that are primarily sourced through Google Scholar to ground the project in established methodologies.\n",
        "\n",
        "\n",
        "* **Data Source**\n",
        "The analysis will utilize the renowned Wine Quality dataset from the UCI Machine Learning Repository. This dataset is composed of two separate files for red and white wine varieties containing 1,599 and 4,898 samples respectively. Each sample is described by 11 physicochemical attributes (e.g., fixed acidity, alcohol) and a single quality score. The data is well-structured, complete and contains no missing values(Cortez et al., 2009).\n",
        "\n",
        "\n",
        "* **Computational Environment**\n",
        "All development and analysis will primarily take place within Google Colab, a cloud-based platform that provides free access to significant computational resources including ample RAM (≈12.5 GB) and optional GPU/TPU hardware acceleration for demanding tasks. Google Drive will be integrated for seamless data storage and access. A local M3-powered device with 16 GB of RAM will serve as a supplementary resource for offline development and debugging.\n",
        "\n",
        "\n",
        "* **Software & Libraries**\n",
        "   The project will be implemented in Python (v3.11.13) which is the default kernel of colab and python(v3.12.2) will be used for offline debugging. The other computing \n",
        "    stack will include:\n",
        "   - **Data Manipulation**: pandas and numpy for efficient data handling.\n",
        "   - **Visualization**: matplotlib and seaborn for exploratory data analysis.\n",
        "   - **Machine Learning Models**: scikit-learn for building the modeling pipeline and XGBoost and LightGBM for implementing advanced gradient boosting algorithms to maximize predictive performance.\n",
        "   - Version control will be managed using GitHub to ensure a reproducible and well-documented workflow.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EtZ510Vd9r-H"
      },
      "source": [
        "# 2. Stage  Two - Data Understanding <a class=\"anchor\"></a>\n",
        "The second stage of the CRISP-DM process requires you to acquire the data listed in the project resources. This initial collection includes data loading, if this is necessary for data understanding. For example, if you use a specific tool for data understanding, it makes perfect sense to load your data into this tool. If you acquire multiple data sources then you need to consider how and when you're going to integrate the various sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i10LYwYq9r-H"
      },
      "source": [
        "## 2.1 Initial Data Acquisition <a class=\"anchor\"></a>\n",
        "List the data sources acquired together with their locations, the methods used to acquire them and any problems encountered. Record problems you encountered and any resolutions achieved. This will help both with future replication of this project and with the execution of similar projects. Ensure you are clear about the various ways in which you can import data into your Notebook. Data can ne read directly from the source e.g. Website or uploaded into the notebook from your computer or cloud storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_6RVBpQI9r-I"
      },
      "outputs": [],
      "source": [
        "# Import Libraries Required\n",
        "#import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "#import numpy as np\n",
        "#import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vRu3idD59r-U"
      },
      "outputs": [],
      "source": [
        "#Data source: \n",
        "#Source Query location: \n",
        "#path =  'F:/Projects/Data Science/Defaults/train_/train.csv' or URL \n",
        "# reads the data from the file - denotes as CSV, it has no header, sets column headers\n",
        "#df =  pd.read_csv(path, sep=',') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ULd3-Y7r9r-X"
      },
      "source": [
        "## 2.2 Describe Data <a class=\"anchor\"></a>\n",
        "Data description of the data that has been acquired including its format, its quantity (for example, the number of records and fields in each table), the identities of the fields and any other surface features which have been discovered. Evaluate whether the data acquired satisfies your requirements to solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9djkYjgm9r-X"
      },
      "outputs": [],
      "source": [
        "#df.columns, df.shape, df.dtypes, df.describe(), df.info() and df.head(10) Use Pandas to explore and clean up your tabular data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hn48zfvE9r-a"
      },
      "source": [
        "## 2.3 Verify Data Quality <a class=\"anchor\"></a>\n",
        "\n",
        "Examine the quality of the data:\n",
        "\n",
        "- Is the data complete (does it cover all that you require)?\n",
        "- Is it correct, or does the data contain errors ?\n",
        "- Are there missing values in the data? If so, where do they occur?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D_xLibVB9r-b"
      },
      "source": [
        "### 2.3.1. Outliers <a class=\"anchor\"></a>\n",
        "At this point, we may also want to remove any outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values or rare events. However, you would remove anomalies based on the definition of extreme outliers:\n",
        "\n",
        "https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm\n",
        "\n",
        "- Below the first quartile − 3 ∗ interquartile range\n",
        "- Above the third quartile + 3 ∗ interquartile range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u5jagtIN9r-b"
      },
      "source": [
        "## 2.4 Initial Data Exploration  <a class=\"anchor\"></a>\n",
        "During this stage, address data questions using querying, data visualization and reporting techniques. These may include:\n",
        "\n",
        "- **Distribution** of key attributes (for example, the target attribute of a prediction task)\n",
        "- **Relationships** between pairs or small numbers of attributes\n",
        "- Results of **simple aggregations**\n",
        "- **Properties** of significant sub-populations\n",
        "- **Simple** statistical analyses\n",
        "\n",
        "These analyses may contribute to or refine the data description and quality aspects of your report, and feed into other data preparation steps needed for further analysis. \n",
        "\n",
        "- **Data exploration component of your report** - Describe results of your data exploration, including first findings or initial hypothesis and their impact on the remainder of the project. Include graphs and plots here to indicate data characteristics that suggest further examination of interesting data subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-PuG4fY59r-c"
      },
      "source": [
        "### 2.4.1 Distributions  <a class=\"anchor\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wZjwoFSt9r-d"
      },
      "outputs": [],
      "source": [
        "def count_values_table(df):\n",
        "        count_val = df.value_counts()\n",
        "        count_val_percent = 100 * df.value_counts() / len(df)\n",
        "        count_val_table = pd.concat([count_val, count_val_percent.round(1)], axis=1)\n",
        "        count_val_table_ren_columns = count_val_table.rename(\n",
        "        columns = {0 : 'Count Values', 1 : '% of Total Values'})\n",
        "        return count_val_table_ren_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6rXORJz39r-g"
      },
      "outputs": [],
      "source": [
        "# Histogram\n",
        "def hist_chart(df, col):\n",
        "        plt.style.use('fivethirtyeight')\n",
        "        plt.hist(df[col].dropna(), edgecolor = 'k');\n",
        "        plt.xlabel(col); plt.ylabel('Number of Entries'); \n",
        "        plt.title('Distribution of '+col);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qpSnggtw9r-j"
      },
      "outputs": [],
      "source": [
        "# col = 'account_risk_band'\n",
        "# Histogram & Results\n",
        "# hist_chart(df, col)\n",
        "# count_values_table(df.account_risk_band)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3FVZDz7I9r-m"
      },
      "source": [
        "### 2.4.2 Correlations  <a class=\"anchor\"></a>\n",
        "Can we derive any correlation from this data-set. Pairplot chart gives us correlations, distributions and regression path\n",
        "Correlogram are awesome for exploratory analysis. It allows to quickly observe the relationship between every variable of your matrix. \n",
        "It is easy to do it with seaborn: just call the pairplot function\n",
        "\n",
        "Pairplot documentation is found here: https://seaborn.pydata.org/generated/seaborn.pairplot.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CKo-HD0m9r-n"
      },
      "outputs": [],
      "source": [
        "#Seaborn allows to make a correlogram or correlation matrix really easily. \n",
        "#sns.pairplot(df.dropna().drop(['x'], axis=1), hue='y', kind ='reg')\n",
        "\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6Ckxwktv9r-p"
      },
      "outputs": [],
      "source": [
        "#df_agg = df.drop(['x'], axis=1).groupby(['y']).sum()\n",
        "#df_agg = df.groupby(['y']).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YTYwzdtU9r-s"
      },
      "source": [
        "# 3. Stage Three - Data Preparation <a class=\"anchor\"></a>\n",
        "This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your data mining goals, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jSnIzuW39r-s"
      },
      "source": [
        "## 3.1 Select Your Data <a class=\"anchor\"></a>\n",
        "This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your machine learning goal, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table.\n",
        "\n",
        "Rationale for inclusion/exclusion - List the data to be included/excluded and the reasons for these decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7fMq8Mfe9r-u"
      },
      "outputs": [],
      "source": [
        "X_train_regr = df.drop(['date_maint', 'account_open_date'], axis = 1)\n",
        "X_train = df.drop(['target', 'date_maint', 'account_open_date'], axis = 1)\n",
        "X_test = test.drop(['date_maint', 'account_open_date'], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4p9JwKkU9r-w"
      },
      "source": [
        "## 3.2 Clean The Data <a class=\"anchor\"></a>\n",
        "This task involves raising the data quality to the level required by the analysis techniques that you've selected. This may involve selecting clean subsets of the data, the insertion of suitable defaults, or more ambitious techniques such as the estimation of missing data by modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dirqThh89r-w"
      },
      "source": [
        "# 4. Stage Four - Modelling <a class=\"anchor\"></a>\n",
        "As the first step in modelling, you'll select the actual modelling technique that you'll be using e.g.Decision tree\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tAFdU3DZ9r-x"
      },
      "source": [
        "## 4.1. Modelling technique <a class=\"anchor\"></a>\n",
        "Document the actual modelling technique that is to be used.\n",
        "\n",
        "Import Models in your code below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B3oshQ_J9r-x"
      },
      "source": [
        "## 4.2. Modelling assumptions <a class=\"anchor\"></a>\n",
        "Many modelling techniques make specific assumptions about the data, for example that all attributes have uniform distributions, no missing values allowed, class attribute must be symbolic etc. Record any assumptions made.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EBP69t3Q9r-y"
      },
      "source": [
        "## 4.3. Build Model <a class=\"anchor\"></a>\n",
        "Run the modelling tool on the prepared dataset to create your model.\n",
        "\n",
        "**Parameter settings** - With any modelling tool there are often a large number of parameters that can be adjusted. List the parameters and their chosen values, along with the rationale for the choice of parameter settings.\n",
        "\n",
        "**Model** - This is the actual model produced by the modelling tool, not a report on the model.\n",
        "\n",
        "**Model description** - Describe the resulting model, report on the interpretation of the model and document any difficulties encountered with their meanings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WChMZ2Uk9r-y"
      },
      "source": [
        "## 4.4. Assess Model <a class=\"anchor\"></a>\n",
        "Interpret the models according to your knowledge, your prediction success criteria and your desired test design. Judge the success of the application of modelling and discovery techniques technically to discuss the machine learning results in the business context. This task only considers models, whereas the evaluation phase also takes into account all other results that were produced in the course of the project.\n",
        "\n",
        "At this stage you should rank the models and assess them according to the evaluation criteria. You should take the business objectives and business success criteria into account as far as you can here. In most ML projects a single technique is applied more than once and results are generated with several different techniques. \n",
        "\n",
        "**Model assessment** - Summarise the results of this task, list the qualities of your generated models (e.g.in terms of accuracy) and rank their quality in relation to each other.\n",
        "\n",
        "**Revised parameter settings** - According to the model assessment, revise parameter settings and tune them for the next modelling run. Iterate model building and assessment until you strongly believe that you have found the best model. Document all such revisions and assessments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s2xPWHz-9r-z"
      },
      "source": [
        "# 5. Stage 5 - Evaluate  <a class=\"anchor\"></a>\n",
        "Previous steps deal with the accuracy and generality of the model. During this step you should assesses the degree to which the model meets your business objectives and seek to determine if there is some business reason why this model is deficient. \n",
        "\n",
        "Assessment of machine learning results - Summarise assessment results in terms of business success criteria, including a final statement regarding whether the project meets the initial business objectives.\n",
        "Approved models - After assessing models with respect to business success criteria, the generated models that meet the selected criteria become the approved models. For this initial assessment, you are only required to consider one model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l4kiQur79r-z"
      },
      "source": [
        "# 6. Stage 6 - Deploy  <a class=\"anchor\"></a>\n",
        "\n",
        "In the deployment stage you would determine a strategy for their deployment and document here together with ongoing monitoring and maintenance of your model. This is particularly important as a predictive machine learning model significantly impacts business operations. For the purposes of this assessment we will use this section to conclude the report. The previous steps should contain your code and narrative text inserted at the relevant sections. Here, you should look at lessons learnt. This includes the things that went right, what went wrong, what you did well and areas for improvement. Additionally, summarise any other expereinces during the project.   \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of CRISP_DM_Template (assessment 2 classification).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
