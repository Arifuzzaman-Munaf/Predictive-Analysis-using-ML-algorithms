{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ItWhqRu49r9_"
      },
      "source": [
        "# Assessment 1: Regression Analysis \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ctR3Nfz9r-D"
      },
      "source": [
        "# 1. Stage One - Determine Business Objectives and Assess the Situation  <a class=\"anchor\"></a>\n",
        "In the modern wine industry, the quality of the product is the paramount factor which determines the success in the market and brand reputation. The existing quality certification practices rely on the application of physicochemical tests and supplemented by sensory tests that are carried out by human experts (Nebot et al., 2015). This traditional approach creates a significant business problem as it fully relies on subjective human taste and the process is also expensive. Moreover, these tests occur at the end of the production cycle when a huge portion of time and money have already been invested. This late quality assessment makes the discovery of a poor-quality batch particularly costly  as the entire production process may need to be repeated. <br>\n",
        "\n",
        "This project directly addresses the inefficiency and financial risk of this reactive quality control model. The objective of the project is to design a predictive analytics model based on the machine learning (ML) techniques using the well-known <a href=\"https://archive.ics.uci.edu/dataset/186/wine+quality\">Wine Quality</a> dataset from the UCI Machine Learning Repository to estimate the quality of wine(ranging from 0 to 10) based on its objective physicochemical characteristics. Such a model will help solve the business problem by transforming quality assessment from a subjective, lagging indicator into an objective, data-driven decision support system. Consequently, winemakers could easily keep track and control quality trends during the production time, referring to the model findings in order to make timely modifications, improving the end product and minimizing the risk of creating low-quality batches (Nebot et al., 2015).\n",
        "\n",
        "## 1.1 Success Criteria\n",
        "Technical effectiveness of the project is measured by three main criteria which correlate modeling results with a real business value.\n",
        "* **Benchmark Predictive Accuracy**: A predictive model will be developed using advanced algorithms (e.g., Gradient Boosting) that achieves a Mean Absolute Error (MAE) of less than 0.50. This performance is required to represent at least a 15% accuracy improvement over a standard linear regression baseline\n",
        "* **Actionable Feature Extraction**: The project will have to determine and prioritize the key physicochemical determinants of wine quality, such as alcohol levels, acidity, and sulphates. This analytical insights gives winemakers the intelligence needed to maximise production by making targeted decisions.\n",
        "* **Robust Multi-Metric Validation**: Model superiority will be confirmed through robust validation using two key metrics. Performance will be measured using Mean Absolute Error(MAE) in order to make it easy to interpret business. Besides, the Coefficient of Determination (R-squared) will be utilized to confirm the ability of the model to explain variance in quality scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q7RZpV__9r-E"
      },
      "source": [
        "## 1.2 Assess the Current Situation<a class=\"anchor\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zv2qPfF99r-G"
      },
      "source": [
        "List the resources available to the project including:\n",
        "\n",
        "* **Personnel**: The project will be conducted by Md. Arifuzzaman Munaf, a postgraduate student specializing in Artificial Intelligence. Responsibilities include data acquisition, exploratory analysis, model building, evaluation, and reporting. The project will also draw on open research, such as the original UCI dataset documentation and published academic analyses on wine quality prediction using machine learning.\n",
        "* **Data**: The dataset comes from the UCI Machine Learning Repository and comprises two files: <br>\n",
        "\t*\t**winequality-red.csv** – 1,599 samples of red wine\n",
        "\t*\t**winequality-white.csv** – 4,898 samples of white wine<br>\n",
        "\n",
        "    Both datasets contain 11 physicochemical input variables and one target variable. The data is clean and well-structured, with no missing values, and has    been widely used in literature for regression and classification experiments (Cortez et al., 2009).\n",
        "\n",
        "\n",
        "* **Computing resources**: The project will primarily use Google Colab, a cloud-based Jupyter notebook platform with access to:\n",
        "\t*\t≈12.5 GB RAM and up to 2 vCPUs\n",
        "\t*\tOptional GPU/TPU acceleration if needed\n",
        "\t*\tGoogle Drive integration for version control\n",
        "\n",
        "    If necessary, a local device with 16 GB RAM and M3 processor 8-core CPU can be used for smaller tests or debugging.\n",
        "* **Software**: The project will use Python 3.11.13 in the Colab environment. The following packages will support the machine learning workflow:\n",
        "\t*\tpandas, numpy: data handling and numerical operations\n",
        "\t*\tmatplotlib, seaborn: visualization and exploratory data analysis\n",
        "\t*\tscikit-learn: regression models, evaluation, and pipeline design\n",
        "\t*\txgboost, lightgbm: gradient boosting models for improved performance\n",
        "\n",
        "    The project will also use GitHub for version control and Google Scholar for referencing relevant academic papers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EtZ510Vd9r-H"
      },
      "source": [
        "# 2. Stage  Two - Data Understanding <a class=\"anchor\"></a>\n",
        "The second stage of the CRISP-DM process requires you to acquire the data listed in the project resources. This initial collection includes data loading, if this is necessary for data understanding. For example, if you use a specific tool for data understanding, it makes perfect sense to load your data into this tool. If you acquire multiple data sources then you need to consider how and when you're going to integrate the various sources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i10LYwYq9r-H"
      },
      "source": [
        "## 2.1 Initial Data Acquisition <a class=\"anchor\"></a>\n",
        "List the data sources acquired together with their locations, the methods used to acquire them and any problems encountered. Record problems you encountered and any resolutions achieved. This will help both with future replication of this project and with the execution of similar projects. Ensure you are clear about the various ways in which you can import data into your Notebook. Data can ne read directly from the source e.g. Website or uploaded into the notebook from your computer or cloud storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_6RVBpQI9r-I"
      },
      "outputs": [],
      "source": [
        "# Import Libraries Required\n",
        "#import pandas as pd\n",
        "#import matplotlib.pyplot as plt\n",
        "#import numpy as np\n",
        "#import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "vRu3idD59r-U"
      },
      "outputs": [],
      "source": [
        "#Data source: \n",
        "#Source Query location: \n",
        "#path =  'F:/Projects/Data Science/Defaults/train_/train.csv' or URL \n",
        "# reads the data from the file - denotes as CSV, it has no header, sets column headers\n",
        "#df =  pd.read_csv(path, sep=',') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ULd3-Y7r9r-X"
      },
      "source": [
        "## 2.2 Describe Data <a class=\"anchor\"></a>\n",
        "Data description of the data that has been acquired including its format, its quantity (for example, the number of records and fields in each table), the identities of the fields and any other surface features which have been discovered. Evaluate whether the data acquired satisfies your requirements to solve the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9djkYjgm9r-X"
      },
      "outputs": [],
      "source": [
        "#df.columns, df.shape, df.dtypes, df.describe(), df.info() and df.head(10) Use Pandas to explore and clean up your tabular data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hn48zfvE9r-a"
      },
      "source": [
        "## 2.3 Verify Data Quality <a class=\"anchor\"></a>\n",
        "\n",
        "Examine the quality of the data:\n",
        "\n",
        "- Is the data complete (does it cover all that you require)?\n",
        "- Is it correct, or does the data contain errors ?\n",
        "- Are there missing values in the data? If so, where do they occur?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D_xLibVB9r-b"
      },
      "source": [
        "### 2.3.1. Outliers <a class=\"anchor\"></a>\n",
        "At this point, we may also want to remove any outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values or rare events. For this assessment, you don't need to worry about outliers. However, you would remove anomalies based on the definition of extreme outliers:\n",
        "\n",
        "https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm\n",
        "\n",
        "- Below the first quartile − 3 ∗ interquartile range\n",
        "- Above the third quartile + 3 ∗ interquartile range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u5jagtIN9r-b"
      },
      "source": [
        "## 2.4 Initial Data Exploration  <a class=\"anchor\"></a>\n",
        "During this stage, address data questions using querying, data visualization and reporting techniques. These may include:\n",
        "\n",
        "- **Distribution** of key attributes (for example, the target attribute of a prediction task)\n",
        "- **Relationships** between pairs or small numbers of attributes\n",
        "- Results of **simple aggregations**\n",
        "- **Properties** of significant sub-populations\n",
        "- **Simple** statistical analyses\n",
        "\n",
        "These analyses may contribute to or refine the data description and quality aspects of your report, and feed into other data preparation steps needed for further analysis. \n",
        "\n",
        "- **Data exploration component of your report** - Describe results of your data exploration, including first findings or initial hypothesis and their impact on the remainder of the project. Include graphs and plots here to indicate data characteristics that suggest further examination of interesting data subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-PuG4fY59r-c"
      },
      "source": [
        "### 2.4.1 Distributions  <a class=\"anchor\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wZjwoFSt9r-d"
      },
      "outputs": [],
      "source": [
        "def count_values_table(df):\n",
        "        count_val = df.value_counts()\n",
        "        count_val_percent = 100 * df.value_counts() / len(df)\n",
        "        count_val_table = pd.concat([count_val, count_val_percent.round(1)], axis=1)\n",
        "        count_val_table_ren_columns = count_val_table.rename(\n",
        "        columns = {0 : 'Count Values', 1 : '% of Total Values'})\n",
        "        return count_val_table_ren_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6rXORJz39r-g"
      },
      "outputs": [],
      "source": [
        "# Histogram\n",
        "def hist_chart(df, col):\n",
        "        plt.style.use('fivethirtyeight')\n",
        "        plt.hist(df[col].dropna(), edgecolor = 'k');\n",
        "        plt.xlabel(col); plt.ylabel('Number of Entries'); \n",
        "        plt.title('Distribution of '+col);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qpSnggtw9r-j"
      },
      "outputs": [],
      "source": [
        "# col = 'account_risk_band'\n",
        "# Histogram & Results\n",
        "# hist_chart(df, col)\n",
        "# count_values_table(df.account_risk_band)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3FVZDz7I9r-m"
      },
      "source": [
        "### 2.4.2 Correlations  <a class=\"anchor\"></a>\n",
        "Can we derive any correlation from this data-set. Pairplot chart gives us correlations, distributions and regression path\n",
        "Correlogram are awesome for exploratory analysis. It allows to quickly observe the relationship between every variable of your matrix. \n",
        "It is easy to do it with seaborn: just call the pairplot function\n",
        "\n",
        "Pairplot documentation is found here: https://seaborn.pydata.org/generated/seaborn.pairplot.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CKo-HD0m9r-n"
      },
      "outputs": [],
      "source": [
        "#Seaborn allows to make a correlogram or correlation matrix really easily. \n",
        "#sns.pairplot(df.dropna().drop(['x'], axis=1), hue='y', kind ='reg')\n",
        "\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "6Ckxwktv9r-p"
      },
      "outputs": [],
      "source": [
        "#df_agg = df.drop(['x'], axis=1).groupby(['y']).sum()\n",
        "#df_agg = df.groupby(['y']).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YTYwzdtU9r-s"
      },
      "source": [
        "# 3. Stage Three - Data Preparation <a class=\"anchor\"></a>\n",
        "This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your data mining goals, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jSnIzuW39r-s"
      },
      "source": [
        "## 3.1 Select Your Data <a class=\"anchor\"></a>\n",
        "This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your machine learning goal, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table.\n",
        "\n",
        "Rationale for inclusion/exclusion - List the data to be included/excluded and the reasons for these decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7fMq8Mfe9r-u"
      },
      "outputs": [],
      "source": [
        "X_train_regr = df.drop(['date_maint', 'account_open_date'], axis = 1)\n",
        "X_train = df.drop(['target', 'date_maint', 'account_open_date'], axis = 1)\n",
        "X_test = test.drop(['date_maint', 'account_open_date'], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4p9JwKkU9r-w"
      },
      "source": [
        "## 3.2 Clean The Data <a class=\"anchor\"></a>\n",
        "This task involves raising the data quality to the level required by the analysis techniques that you've selected. This may involve selecting clean subsets of the data, the insertion of suitable defaults, or more ambitious techniques such as the estimation of missing data by modelling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dirqThh89r-w"
      },
      "source": [
        "# 4. Stage Four - Modelling <a class=\"anchor\"></a>\n",
        "As the first step in modelling, you'll select the actual modelling technique that you'll be using e.g.Linear Regression \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tAFdU3DZ9r-x"
      },
      "source": [
        "## 4.1. Modelling technique <a class=\"anchor\"></a>\n",
        "Document the actual modelling technique that is to be used.\n",
        "\n",
        "Import Models in your code below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B3oshQ_J9r-x"
      },
      "source": [
        "## 4.2. Modelling assumptions <a class=\"anchor\"></a>\n",
        "Many modelling techniques make specific assumptions about the data, for example that all attributes have uniform distributions, no missing values allowed, class attribute must be symbolic etc. Record any assumptions made.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EBP69t3Q9r-y"
      },
      "source": [
        "## 4.3. Build Model <a class=\"anchor\"></a>\n",
        "Run the modelling tool on the prepared dataset to create your model.\n",
        "\n",
        "**Parameter settings** - With any modelling tool there are often a large number of parameters that can be adjusted. List the parameters and their chosen values, along with the rationale for the choice of parameter settings.\n",
        "\n",
        "**Model** - This is the actual model produced by the modelling tool, not a report on the model.\n",
        "\n",
        "**Model description** - Describe the resulting model, report on the interpretation of the model and document any difficulties encountered with their meanings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WChMZ2Uk9r-y"
      },
      "source": [
        "## 4.4. Assess Model <a class=\"anchor\"></a>\n",
        "Interpret the models according to your knowledge, your prediction success criteria and your desired test design. Judge the success of the application of modelling and discovery techniques technically to discuss the machine learning results in the business context. This task only considers models, whereas the evaluation phase also takes into account all other results that were produced in the course of the project.\n",
        "\n",
        "At this stage you should rank the models and assess them according to the evaluation criteria. You should take the business objectives and business success criteria into account as far as you can here. In most ML projects a single technique is applied more than once and results are generated with several different techniques. \n",
        "\n",
        "**Model assessment** - Summarise the results of this task, list the qualities of your generated models (e.g.in terms of accuracy) and rank their quality in relation to each other.\n",
        "\n",
        "**Revised parameter settings** - According to the model assessment, revise parameter settings and tune them for the next modelling run. Iterate model building and assessment until you strongly believe that you have found the best model. Document all such revisions and assessments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s2xPWHz-9r-z"
      },
      "source": [
        "# 5. Stage 5 - Evaluate  <a class=\"anchor\"></a>\n",
        "Previous steps deal with the accuracy and generality of the model. During this step you should assesses the degree to which the model meets your business objectives and seek to determine if there is some business reason why this model is deficient. \n",
        "\n",
        "Assessment of machine learning results - Summarise assessment results in terms of business success criteria, including a final statement regarding whether the project meets the initial business objectives.\n",
        "Approved models - After assessing models with respect to business success criteria, the generated models that meet the selected criteria become the approved models. For this initial assessment, you are only required to consider one model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l4kiQur79r-z"
      },
      "source": [
        "# 6. Stage 6 - Deploy  <a class=\"anchor\"></a>\n",
        "\n",
        "In the deployment stage you would determine a strategy for their deployment and document here together with ongoing monitoring and maintenance of your model. This is particularly important as a predictive machine learning model significantly impacts business operations. For the purposes of this assessment we will use this section to conclude the report. The previous steps should contain your code and narrative text inserted at the relevant sections. Here, you should look at lessons learnt. This includes the things that went right, what went wrong, what you did well and areas for improvement. Additionally, summarise any other expereinces during the project.   \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CRISP-DM Template.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
